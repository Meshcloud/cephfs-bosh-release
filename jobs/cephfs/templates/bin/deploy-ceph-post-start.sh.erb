#!/bin/bash

set -x

PWD=`pwd`
user=vcap
hostname=`hostname`

ipaddr=$(ip addr | grep 'state UP' -A2 | tail -n1 | awk '{print $2}' | cut -f1  -d'/')
ifcace=$(ifconfig | grep encap:Ethernet | cut -d" " -f1)
DEPLOYMENT=<%= spec.deployment %>
firstadress=<%= (link("ceph-storage").instances.first.address) %>
HOSTNAMES=( $(cat /var/vcap/instance/dns/records.json | jq '.record_infos | .[] | {'host':.[11], 'id':.[12], 'deployment_name':.[8]}' | jq  -r --arg DEPLOYMENT "$DEPLOYMENT" 'select(.deployment_name==$DEPLOYMENT)' | jq --raw-output --slurp '.=sort_by(.id) | .[].host') )
IPADDRESSES=( $(cat /var/vcap/instance/dns/records.json | jq '.record_infos | .[] | {'ip':.[9], 'id':.[12], 'deployment_name':.[8]}' | jq  -r --arg DEPLOYMENT "$DEPLOYMENT" 'select(.deployment_name==$DEPLOYMENT)' | jq --raw-output --slurp '.=sort_by(.id) | .[].ip') )
NETWORK='<%= p("vm.network") %>'

if [ "$hostname" == "${HOSTNAMES[0]}" ]; then 
	mkdir /root/ceph-cluster
	cd /root/ceph-cluster 

	echo "Deploying monitor to $ipaddr"
	ceph-deploy new $(echo ${HOSTNAMES[0]} ) #make the admin Node to the Monitor node

	if [ ! -e "/root/ceph-cluster/ceph.conf" ]; then
        echo "Error while deploying monitor. File ceph.conf is missing - Exiting!"
        exit 1
	fi


	for hostname in "${HOSTNAMES[@]}"
	do
	 ceph-deploy install $(echo $hostname)
	done	

	echo "public network = $NETWORK" | tee -a ceph.conf 

	ceph-deploy --overwrite-conf mon create-initial

	if [ ! -e "/root/ceph-cluster/ceph.client.admin.keyring" ]; then
        echo "Error while creating initial monitor. keyring missing. Exiting!"
        exit 1
	fi

	for hostname in "${HOSTNAMES[@]}"
	do
	 ceph-deploy admin $(echo $hostname)
	done	


	##Creating OSDS

	diskfile=$(sudo lsblk | grep /var/vcap/store |  cut -d" " -f1 | sed 's/^..//' | sed 's/.$//')
	$(su vcap)
	
	for hostname in "${HOSTNAMES[@]}"
	do
	 ssh $hostname sudo umount -f /var/vcap/store	
	 ceph-deploy disk zap $hostname:$diskfile
	 ceph-deploy osd prepare $hostname:$diskfile
	 ceph-deploy osd activate $hostname:$diskfile
	done	

	ceph health > /var/vcap/sys/log/cephfs/health

	#Creating Metadata Server 
	##We choose the Second Server of our cluster to be the Metada Server

	ceph-deploy mds create ${HOSTNAMES[1]}
	ceph-deploy install ceph-client ${HOSTNAMES[1]}

	#Creating additional monitorsST
	for hostname in "${HOSTNAMES[@]:1:$((${#HOSTNAMES[@]} / 2))-1}"
	do 
		ceph-deploy mon add $hostname
	done 
	#Deploy CephFS
	##Create OSD Pools

	ceph osd pool create cephfs_data 128
	ceph osd pool create cephfs_metadata 128
	ceph fs new cephfs cephfs_metadata cephfs_data
	ceph mds stat
fi	



exit 0
